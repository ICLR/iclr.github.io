<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title> ICLR: VL-BERT: Pre-training of Generic Visual-Linguistic Representations </title>
</head>

  <body >
    
<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">

<style>
    body{font-family: 'Lato', sans-serif;}
</style>

<nav class="navbar navbar-expand-lg navbar-light bg-light mr-auto">
  <a class="navbar-brand" href="#">
    <img class="logo" src="https://www.iclr.cc/static/admin/img/ICLR-logo.png"  height="35"/>
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">
        <a class="nav-link" href="index.html">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="livestream.html">Live Stream</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="papers.html">Papers</a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="paper_vis.html">PaperVis</a>
      </li>










      </ul>
</div>
  </div>
</nav>


<div class="container">
  
 <!-- Title -->

<div class="card" >
  <div class="card-body">
    <h3 class="card-title">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</h3>
    <h6 class="card-subtitle mb-2 text-muted">
      Weijie Su,
      
      Xizhou Zhu,
      
      Yue Cao,
      
      Bin Li,
      
      Lewei Lu,
      
      Furu Wei,
      
      Jifeng Dai,
      
    </h6>
    <p class="card-text"><span class="font-weight-bold">TL;DR:</span> VL-BERT is a simple yet powerful pre-trainable generic representation for visual-linguistic tasks. It is pre-trained on the massive-scale caption dataset and text-only corpus, and can be finetuned for varies down-stream visual-linguistic tasks.</p>
    <p class="card-text"><span class="font-weight-bold">Abstract:</span> We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark.</p>
    
    <center>
      <a class="card-link"  href="http://www.openreview.net/pdf/82a3e5b930bb31fb2830baf68da549d0431c8209.pdf">Paper</a>
      <a class="card-link"  href="http://www.openreview.net/forum?id=SygXPaEYvH">OpenReview</a>
    <!-- <span><a href="" class="btn btn-secondary">OpenReview</a></span> -->

      <a href="https://github.com/jackroos/VL-BERT" class="card-link">Code</a>
      <a href="" class="card-link">Slides</a>
    </center>
    <p></p>
    <p class="card-text"><span class="font-weight-bold">Keywords:</span>
      
      <a href="keyword_Visual-Linguistic.html" class="text-secondary text-decoration-none">Visual-Linguistic</a>,
      
      <a href="keyword_Generic Representation.html" class="text-secondary text-decoration-none">Generic Representation</a>,
      
      <a href="keyword_Pre-training.html" class="text-secondary text-decoration-none">Pre-training</a>,
      
    </p>    
  </div>
</div>

<div>
</div>

</div>


<!-- SlidesLive -->
<div id="presentation-embed-38915748" class="container container-sm"></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
  embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true
    });
</script>


<!-- Buttons -->
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
<!--   <center> -->
<!--     <span><a href="poster_.html" class="btn btn-secondary">Prev</a></span> -->

<!--     <span><a href="" class="btn btn-secondary">Video Call</a></span> -->

<!--     <span><a href="poster_.html" class="btn btn-secondary">Next</a></span> </center> -->
<!-- </div> -->
<!--   <center> -->
<center>
    <h2> Paper Discussion       </h2>
    <span><a class="btn btn-secondary" href="https://gitter.im/iclr/posterSygXPaEYvH/">Chat</a></span>
</center>
<p></p>
<!-- Gitter -->
<div id="gitter" class="gitter container" height="600px">
  <center>
    <div class="border">
      <center> <iframe frameborder="0" src="https://gitter.im/iclr/posterSygXPaEYvH/~embed" width="900px" height="400px"></iframe> </center>
    </div>
  </center>
</div>

<!-- Recs -->
<p></p>
<div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
    <div class="card-deck">

      
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_BJgQ4lSFPH.html" class="text-dark"><h5 class="card-title">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment clas...</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_BJgQ4lSFPH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_HJlWWJSFDH.html" class="text-dark"><h5 class="card-title">Strategies for Pre-training Graph Neural Networks</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_HJlWWJSFDH.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_HkxYzANYDB.html" class="text-dark"><h5 class="card-title">CLEVRER: Collision Events for Video Representation and Reasoning</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. </p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_HkxYzANYDB.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
      
  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_Hyl7ygStwB.html" class="text-dark"><h5 class="card-title">Incorporating BERT into Neural Machine Translation</h5></a>      

    </div>
      <div class="card-body">
        <p class="card-text"> The recently proposed BERT (Devlin et al., 2019) has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation...</p>
      </div>
      
      <div class="card-footer">
      <center>
        <a href="poster_Hyl7ygStwB.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
    </DIV>
          </DIV>
      </DIV>

</body>